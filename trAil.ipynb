{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc71753c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 12, 10000])\n",
      "Generated shape: torch.Size([2, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#######################################\n",
    "# Basic Multi-Head Attention Module\n",
    "#######################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        # Linear projections for Q, K, and V\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Project input embeddings and reshape for multi-head attention: (B, seq_len, embed_dim)\n",
    "        q = self.q_proj(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Multiply attention weights with value vectors\n",
    "        context = torch.matmul(attn, v)  # shape: (B, num_heads, seq_len_q, head_dim)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "        output = self.out_proj(context)\n",
    "        return output\n",
    "\n",
    "#######################################\n",
    "# Transformer Encoder Layer for Text Encoder\n",
    "#######################################\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention (text-only)\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "#######################################\n",
    "# Text Encoder Module\n",
    "#######################################\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, ff_dim, max_seq_len=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Learnable positional embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text: (batch, seq_len)\n",
    "        x = self.embed(text)  # (B, seq_len, embed_dim)\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        return x  # (B, seq_len, embed_dim)\n",
    "\n",
    "#######################################\n",
    "# Image Encoder Module\n",
    "#######################################\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        # A simple CNN to obtain patches from an image sized 256x256.\n",
    "        # Using a kernel_size and stride of 16, we obtain 16x16 = 256 patches.\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=16, stride=16),  # from 256x256 -> 16x16 feature map (64 channels)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, embed_dim, kernel_size=1)\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=2)  # flatten the spatial dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 3, 256, 256)\n",
    "        x = self.conv(x)  # (batch, embed_dim, 16, 16)\n",
    "        x = self.flatten(x).transpose(1, 2)  # (batch, 256, embed_dim) with 256 patches per image\n",
    "        return x\n",
    "\n",
    "#######################################\n",
    "# Fused Knowledge Module\n",
    "#######################################\n",
    "class FusedKnowledge(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        # We use multi-head attention to fuse the text encoder output (query)\n",
    "        # with image encoder output (keys and values)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, text_repr, image_repr, mask=None):\n",
    "        # text_repr: (batch, text_seq_len, embed_dim) used as query\n",
    "        # image_repr: (batch, image_patches, embed_dim) used as key and value\n",
    "        fused = self.cross_attn(text_repr, image_repr, image_repr, mask)\n",
    "        # Add residual connection and layer normalization\n",
    "        fused = self.norm(text_repr + self.dropout(fused))\n",
    "        return fused  # (batch, text_seq_len, embed_dim)\n",
    "\n",
    "#######################################\n",
    "# Transformer Decoder for Captioning\n",
    "#######################################\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, ff_dim, max_seq_len=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        # Tie the output weights with the embedding matrix\n",
    "        self.output_proj = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # tgt: (batch, tgt_seq_len)\n",
    "        x = self.embed(tgt)  # (batch, tgt_seq_len, embed_dim)\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask, memory_mask)\n",
    "        x = self.norm(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "#######################################\n",
    "# Transformer Decoder Layer (for decoder)\n",
    "#######################################\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # Self-attention on the target sequence\n",
    "        self_attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
    "        tgt = tgt + self.dropout(self_attn_output)\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # Cross-attention with memory from the fusion module\n",
    "        cross_attn_output = self.cross_attn(tgt, memory, memory, memory_mask)\n",
    "        tgt = tgt + self.dropout(cross_attn_output)\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(tgt)\n",
    "        tgt = tgt + self.dropout(ffn_output)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "#######################################\n",
    "# Full Multimodal Fusion Model for Image Captioning\n",
    "#######################################\n",
    "class MultimodalFusionModel(nn.Module):\n",
    "    def __init__(self, vocab_size,\n",
    "                 embed_dim=512,\n",
    "                 num_layers_enc=2,\n",
    "                 num_layers_dec=4,\n",
    "                 num_heads=8,\n",
    "                 ff_dim=2048,\n",
    "                 max_seq_len=128):\n",
    "        super().__init__()\n",
    "        # Text encoder for any prompt or auxiliary text input\n",
    "        self.text_encoder = TextEncoder(vocab_size, embed_dim, num_layers_enc, num_heads, ff_dim, max_seq_len)\n",
    "        # Image encoder to extract patch features from 256x256 images\n",
    "        self.image_encoder = ImageEncoder(embed_dim)\n",
    "        # Fusion module to fuse text (query) with image (key,value)\n",
    "        self.fusion = FusedKnowledge(embed_dim, num_heads)\n",
    "        # Decoder to generate captions from fused memory representation\n",
    "        self.decoder = Decoder(vocab_size, embed_dim, num_layers_dec, num_heads, ff_dim, max_seq_len)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, image, tgt, text_input):\n",
    "        \"\"\"\n",
    "        image: tensor of shape (batch, 3, 256, 256)\n",
    "        tgt: caption token sequence (batch, tgt_seq_len) for teacher forcing\n",
    "        text_input: text tokens for the text encoder (batch, text_seq_len)\n",
    "                    This could be a fixed prompt or additional text description.\n",
    "        \"\"\"\n",
    "        # Obtain text and image representations\n",
    "        text_repr = self.text_encoder(text_input)         # (B, text_seq_len, embed_dim)\n",
    "        image_repr = self.image_encoder(image)              # (B, num_patches, embed_dim)\n",
    "\n",
    "        # Fuse text and image features; here text representation serves as queries\n",
    "        fused_memory = self.fusion(text_repr, image_repr)   # (B, text_seq_len, embed_dim)\n",
    "\n",
    "        # Create the causal mask for the decoder (to ensure auto-regressive decoding)\n",
    "        batch_size, tgt_seq_len = tgt.size(0), tgt.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(tgt_seq_len, tgt_seq_len, device=tgt.device), diagonal=1).bool().logical_not()\n",
    "        tgt_padding_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (causal_mask[None, None, :, :] & tgt_padding_mask).to(torch.float)\n",
    "\n",
    "        # Decode captions using the fused memory as encoder output\n",
    "        logits = self.decoder(tgt, fused_memory, tgt_mask=tgt_mask)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, image, text_input, max_len=50):\n",
    "        \"\"\"\n",
    "        Auto-regressively generate captions.\n",
    "        image: (batch, 3, 256, 256)\n",
    "        text_input: (batch, text_seq_len)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        batch_size = image.size(0)\n",
    "        # Compute fused memory from text and image representations\n",
    "        text_repr = self.text_encoder(text_input)\n",
    "        image_repr = self.image_encoder(image)\n",
    "        fused_memory = self.fusion(text_repr, image_repr)\n",
    "\n",
    "        # Initialize generated caption with the <start> token (assumed id=1)\n",
    "        tgt = torch.ones(batch_size, 1, device=image.device, dtype=torch.long) * 1\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits = self.decoder(tgt, fused_memory)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            generated_tokens.append(next_token)\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "            # Check if all sequences predicted the <end> token (assumed id=2)\n",
    "            if (next_token == 2).all():\n",
    "                break\n",
    "\n",
    "        # Concatenate and return generated tokens (without the initial <start>)\n",
    "        return torch.cat(generated_tokens, dim=1)\n",
    "\n",
    "#######################################\n",
    "# Example instantiation and usage\n",
    "#######################################\n",
    "if __name__ == '__main__':\n",
    "    # Assume vocabulary size of 10000 tokens\n",
    "    vocab_size = 10000\n",
    "    model = MultimodalFusionModel(vocab_size)\n",
    "    # Dummy inputs: a batch of 2 images (3,256,256) and text prompt tokens (batch, text_seq_len)\n",
    "    dummy_images = torch.randn(2, 3, 256, 256)\n",
    "    dummy_text = torch.randint(0, vocab_size, (2, 10))  # e.g., a prompt with 10 tokens\n",
    "    dummy_tgt = torch.randint(0, vocab_size, (2, 12))   # target captions for teacher forcing\n",
    "    # Forward pass\n",
    "    logits = model(dummy_images, dummy_tgt, dummy_text)\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "    # Generation (auto-regressive decoding)\n",
    "    generated = model.generate(dummy_images, dummy_text)\n",
    "    print(\"Generated shape:\", generated.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19b0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import the updated multimodal fusion model with two encoders, fusion, and decoder\n",
    "# from ModelFlow import MultimodalFusionModel\n",
    "\n",
    "#############################################\n",
    "# Dataset for Image Captioning\n",
    "#############################################\n",
    "class CaptioningDataset(Dataset):\n",
    "    def __init__(self, json_file, transform=None):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        all_captions = [caption.lower().split() for caption in self.data.values()]\n",
    "        word_freq = Counter(word for caption in all_captions for word in caption)\n",
    "        # Reserve special tokens: <pad>, <start>, <end>, <unk>\n",
    "        self.vocab = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}\n",
    "        # Allow additional tokens (here up to 29996 tokens) from the frequency list\n",
    "        self.vocab.update({word: idx for idx, (word, _) in enumerate(word_freq.most_common(29996), start=4)})\n",
    "        self.idx_to_word = {idx: word for word, idx in self.vocab.items()}\n",
    "        self.image_paths = list(self.data.keys())\n",
    "        self.captions = []\n",
    "        for caption in self.data.values():\n",
    "            words = caption.lower().split()\n",
    "            indices = [self.vocab.get(word, self.vocab['<unk>']) for word in words]\n",
    "            # Surround each caption with start and end tokens\n",
    "            self.captions.append([self.vocab['<start>']] + indices + [self.vocab['<end>']])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        caption = self.captions[idx]\n",
    "        return image, caption\n",
    "\n",
    "#############################################\n",
    "# Collate function to batch data\n",
    "#############################################\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    max_len = max(len(c) for c in captions)\n",
    "    padded_captions = [c + [0] * (max_len - len(c)) for c in captions]\n",
    "    return images, torch.tensor(padded_captions)\n",
    "\n",
    "#############################################\n",
    "# Utility: Denormalize image for visualization\n",
    "#############################################\n",
    "def denormalize(image):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    image = image.cpu() * std + mean\n",
    "    image = image.clamp(0, 1).permute(1, 2, 0).numpy()\n",
    "    return image\n",
    "\n",
    "#############################################\n",
    "# Training Loop\n",
    "#############################################\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Transformation for images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create dataset and compute vocabulary size\n",
    "    dataset = CaptioningDataset('assets/processed_captions.json', transform=transform)\n",
    "    vocab_size = len(dataset.vocab)\n",
    "\n",
    "    # Instantiate the multimodal model.\n",
    "    # Note: MultimodalFusionModel requires three inputs: image, tgt, and text_input.\n",
    "    model = MultimodalFusionModel(vocab_size=vocab_size,\n",
    "                                  embed_dim=512,\n",
    "                                  num_layers_enc=2,  # Number of layers in the text encoder\n",
    "                                  num_layers_dec=4,  # Number of layers in the decoder\n",
    "                                  num_heads=8,\n",
    "                                  ff_dim=2048,\n",
    "                                  max_seq_len=128).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Split dataset into training and validation sets.\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=32,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=collate_fn,\n",
    "                              num_workers=4,\n",
    "                              pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset,\n",
    "                            batch_size=32,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=collate_fn,\n",
    "                            num_workers=4,\n",
    "                            pin_memory=True)\n",
    "\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "\n",
    "    num_epochs = 100\n",
    "\n",
    "    # For this example, we use a fixed text prompt as input to the text encoder.\n",
    "    # You can modify prompt_length as needed.\n",
    "    prompt_length = 10  # e.g. 10 tokens prompt. Here we fill it with the <start> token (assumed id=1).\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, captions in train_loader:\n",
    "            images, captions = images.to(device), captions.to(device)\n",
    "            # Teacher forcing: use caption tokens except the last one as target input.\n",
    "            tgt = captions[:, :-1]\n",
    "            target = captions[:, 1:]\n",
    "\n",
    "            # Create a fixed text prompt. Here we use a vector filled with <start> token (id=1).\n",
    "            batch_size = images.size(0)\n",
    "            text_prompt = torch.ones(batch_size, prompt_length, device=device, dtype=torch.long) * 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                # Forward pass uses three inputs: image, tgt, and text_input (the text prompt)\n",
    "                logits = model(image=images, tgt=tgt, text_input=text_prompt)\n",
    "                loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)),\n",
    "                                       target.reshape(-1),\n",
    "                                       ignore_index=0)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Save checkpoint and visualize predictions every 10 epochs (or on the final epoch)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "            checkpoint_path = f'checkpoints/model_epoch_{epoch+1}.pth'\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f'Model saved to {checkpoint_path}')\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_images, val_captions in val_loader:\n",
    "                    val_images = val_images.to(device)\n",
    "                    # Use the same fixed prompt for generation.\n",
    "                    batch_size = val_images.size(0)\n",
    "                    text_prompt = torch.ones(batch_size, prompt_length, device=device, dtype=torch.long) * 1\n",
    "\n",
    "                    # Generate captions auto-regressively\n",
    "                    generated = model.generate(val_images, text_input=text_prompt, max_len=50)\n",
    "\n",
    "                    samples = []\n",
    "                    # Collect sample outputs to visualize (first 3 samples)\n",
    "                    for i in range(min(3, len(generated))):\n",
    "                        gen_indices = generated[i].tolist()\n",
    "                        gen_words = [dataset.idx_to_word.get(idx, '<unk>')\n",
    "                                     for idx in gen_indices if idx not in [0, 1, 2]]\n",
    "                        gt_indices = val_captions[i].tolist()\n",
    "                        gt_words = [dataset.idx_to_word.get(idx, '<unk>')\n",
    "                                    for idx in gt_indices if idx not in [0, 1, 2]]\n",
    "                        samples.append({\n",
    "                            'image': val_images[i],\n",
    "                            'gen_caption': ' '.join(gen_words),\n",
    "                            'gt_caption': ' '.join(gt_words)\n",
    "                        })\n",
    "                    break\n",
    "\n",
    "                # Plot and save the visualizations\n",
    "                fig, axes = plt.subplots(3, 1, figsize=(8, 24))\n",
    "                for i, sample in enumerate(samples):\n",
    "                    img = denormalize(sample['image'])\n",
    "                    axes[i].imshow(img)\n",
    "                    axes[i].set_title(\n",
    "                        f\"Generated: {sample['gen_caption']}\\nGround Truth: {sample['gt_caption']}\",\n",
    "                        fontsize=10\n",
    "                    )\n",
    "                    axes[i].axis('off')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'visualizations/epoch_{epoch+1}.png')\n",
    "                plt.close()\n",
    "\n",
    "    print(\"Training completed. Model is ready for finetuning on logical reasoning.\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17df08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4348c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
